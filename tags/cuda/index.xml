<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>CUDA on Ettore Saggiorato - Sa1g</title><link>https://sa1g.github.io/tags/cuda/</link><description>Recent content in CUDA on Ettore Saggiorato - Sa1g</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 01 Feb 2025 01:00:00 +0000</lastBuildDate><atom:link href="https://sa1g.github.io/tags/cuda/index.xml" rel="self" type="application/rss+xml"/><item><title>GPU Computing</title><link>https://sa1g.github.io/p/gpu-computing/</link><pubDate>Sat, 01 Feb 2025 01:00:00 +0000</pubDate><guid>https://sa1g.github.io/p/gpu-computing/</guid><description>&lt;p>&lt;a class="link" href="https://github.com/sa1g/gpu-computing" target="_blank" rel="noopener"
>Project Repo&lt;/a>&lt;/p>
&lt;p>These assignments are about learning the basics of GPU programming using CUDA. In all assignment we benchmark non-symmetric matrices of size $N \times N$, where $N$ is a power of 2, with different kernels and block sizes.&lt;/p>
&lt;p align="center">
&lt;a href="https://github.com/sa1g/gpu-computing/blob/main/A3/report/stitched_report.pdf">&lt;img src="image.png" width="250"/>&lt;/a>
&lt;/p>
&lt;p align="center">
Click to download report
&lt;/p>
&lt;h2 id="first-assignment">First Assignment
&lt;/h2>&lt;p>Develop a few algorithms to transpose the matrix in CPU, measure effective bandwidth under different compiler optimizations (-O0, -O1, -O2, -O3), analyze cache behavior and efficiency.&lt;/p>
&lt;h2 id="second-assignment">Second Assignment
&lt;/h2>&lt;p>Develop a few algorithms to transpose the matrix in GPU, measure effective bandwidth and efficiency.
The developed algorithms are:&lt;/p>
&lt;ul>
&lt;li>simple transposition&lt;/li>
&lt;li>with shared memory&lt;/li>
&lt;li>with shared memory and coalesced memory access&lt;/li>
&lt;/ul>
&lt;p>Results were also compared with the Copy of the matrix in GPU, to compare to the &lt;em>simplest&lt;/em> algorithm we could use.&lt;/p>
&lt;h2 id="third-assignment">Third Assignment
&lt;/h2>&lt;p>Develop some algorithms to transpose the matrix in GPU, using NVIDIA&amp;rsquo;s Cooperative Groups. Compare the results against cuBLAS.
This wasn&amp;rsquo;t straightforward as Cooperative Groups are not a good idea for dense matrix transposition, so I had to &lt;em>work against&lt;/em> all the things that were studied during the course.
Three algorithms were developed (ignoring copy):&lt;/p>
&lt;ul>
&lt;li>Intra-block synchronization (good old &lt;code>__syncthreads()&lt;/code>)&lt;/li>
&lt;li>Intra-block synchronization coalesced&lt;/li>
&lt;li>Inter-block synchronization (grid-wide synch, expected and demonstrated slower)&lt;/li>
&lt;/ul></description></item></channel></rss>